{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "home=os.path.expanduser(\"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\400T6B\n"
     ]
    }
   ],
   "source": [
    "print home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(os.path.join(home,\"Downloads\",\"spark-2.0.0-bin-hadoop2.6\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .config('spark.sql.warehouse.dir','file:///C:\\Users\\code\\s_201110216\\data')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).config('spark.sql.warehouse.dir','file:///C:\\Users\\code\\s_201110216\\data').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "117.16.43.198\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n"
     ]
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "print myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc2=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "(7, u'Spark')\n",
      "(6, u'Apache')\n",
      "(5, u'\\uc544\\ud30c\\uce58')\n",
      "(4, u'\\uc2a4\\ud30c\\ud06c')\n",
      "(3, u'the')\n",
      "(2, u'an')\n",
      "(1, u'and')\n",
      "(1, u'\\uc18c\\uc2a4')\n",
      "(1, u'is')\n",
      "(1, u'Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "print type(wc2)\n",
    "for i in wc2:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAD8CAYAAABKKbKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x662e7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = map(lambda x: x[0], wc2)\n",
    "word = map(lambda x: x[1], wc2)\n",
    "plt.barh(range(len(count)), count, color = 'grey')\n",
    "plt.yticks(range(len(count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\python.exe: can't open file 'C:/Users/400T6B/code/s_201110216/code/s_201110216/src/ds_spark_rdd_hello.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!C:\\Users\\400T6B\\Downloads\\spark-2.0.0-bin-hadoop2.6\\bin\\spark-submit code\\s_201110216\\src\\ds_spark_rdd_hello.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------RESULT-----------\n",
      "2.0.0\n",
      "mean= 499.5\n",
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "        \n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").appName(\"myApp\").config(conf=myConf).getOrCreate()\n",
    "doIt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {253068: 1.0}),\n",
       " SparseVector(1048576, {36751: 1.0, 50570: 1.0, 68380: 1.0, 415281: 1.0, 511377: 1.0, 728364: 1.0, 862087: 1.0, 938426: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {63234: 1.0, 340190: 1.0, 357478: 1.0, 375592: 1.0, 458138: 1.0, 486171: 1.0, 598772: 1.0}),\n",
       " SparseVector(1048576, {938426: 4.0, 999480: 4.0}),\n",
       " SparseVector(1048576, {486171: 4.0, 1016271: 4.0}),\n",
       " SparseVector(1048576, {36757: 1.0, 225801: 1.0, 323305: 1.0, 453405: 1.0, 498679: 1.0, 518030: 1.0, 688842: 1.0, 762570: 1.0, 959994: 1.0}),\n",
       " SparseVector(1048576, {420843: 1.0, 550676: 1.0, 725041: 1.0, 782544: 1.0, 938426: 1.0, 959994: 2.0, 991590: 1.0, 993084: 1.0, 996703: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {50573: 1.0, 263739: 1.0, 892834: 1.0, 1014710: 1.0, 1035538: 1.0}),\n",
       " SparseVector(1048576, {3932: 1.0, 36751: 1.0, 192182: 1.0, 358969: 1.0, 363244: 1.0, 496856: 1.0, 546913: 1.0, 938426: 1.0, 951974: 1.0}),\n",
       " SparseVector(1048576, {69621: 1.0, 157580: 1.0, 219357: 1.0, 297436: 1.0, 715648: 1.0})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "tf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print myList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee',170)]\n",
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1=u'1', _2=u'kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "print myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|      item|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  espresso|\n",
      "| lee|     latte|\n",
      "| lim| americano|\n",
      "| kim|  affocato|\n",
      "| lee|long black|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]\n",
    "df = spark.createDataFrame([(names[i%4], items[i%5]) for i in range(100)],\\\n",
    "                           [\"name\",\"item\"])\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|3chars|\n",
      "+------+\n",
      "|   esp|\n",
      "|   lat|\n",
      "|   ame|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.item.substr(1, 3).alias(\"3chars\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)\n",
    "print \"row1: \",row1.year, row1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n",
    "\n",
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(year=u'1', name=u'kim, js', height=170)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()\n",
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "myList=[('1','kim, js',170),('1','lee, sm', 175),('2','lim, yg',180),('2','lee',170)]\n",
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=myRdd.toDF()\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)\n",
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| _1|     _2|\n",
      "+---+-------+\n",
      "|  1|kim, js|\n",
      "|  2|    lee|\n",
      "+---+-------+\n",
      "\n",
      "+---+-------+\n",
      "| _1|max(_3)|\n",
      "+---+-------+\n",
      "|  1|    175|\n",
      "|  2|    180|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rddDf.where(rddDf._3 < 175)\\\n",
    "    .select([rddDf._1, rddDf._2]).show()\n",
    "rddDf.groupby(rddDf._1).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- height: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(height=170, name=u'kim, js', year=1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]),name=x[1],height=int(x[2])))\n",
    "_myDf=spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()\n",
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=10, name='js1'), Row(age=20, name='js2')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType, TimestampType\n",
    "r1=Row(name=\"js1\",age=10)\n",
    "r2=Row(name=\"js2\",age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])\n",
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 10| js1|\n",
      "| 20| js2|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd,schema)\n",
    "_myDf.printSchema()\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim, js</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>lee, sm</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lim, yg</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  year     name  height\n",
       "0    1  kim, js     170\n",
       "1    1  lee, sm     175\n",
       "2    2  lim, yg     180\n",
       "3    2      lee     170"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.txt\")\n",
    "lines = spark.sparkContext.textFile(cfile)\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(people)\n",
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+\n",
      "|  1|  2|  3|  4|\n",
      "+---+---+---+---+\n",
      "| 11| 22| 33| 44|\n",
      "|111|222|333|444|\n",
      "+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/ds_spark.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.658985,  4.285136])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985 4.285136'.split(' ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '.',\n",
       " '6',\n",
       " '5',\n",
       " '8',\n",
       " '9',\n",
       " '9',\n",
       " '5',\n",
       " ' ',\n",
       " '\\t',\n",
       " ' ',\n",
       " '4',\n",
       " '.',\n",
       " '2',\n",
       " '8',\n",
       " '5',\n",
       " '1',\n",
       " '3',\n",
       " '6']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in '1.658995 \t 4.285136']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data\\\\ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data\\\\ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "tDf=spark.createDataFrame(tRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'1', _2=u'65.78', _3=u'112.99'),\n",
       " Row(_1=u'2', _2=u'71.52', _3=u'136.49'),\n",
       " Row(_1=u'3', _2=u'69.40', _3=u'153.03')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tDf=tDf.withColumn(\"id\",tDf['_1'].cast(\"integer\")).drop('_1')\n",
    "tDf=tDf.withColumn(\"height\",tDf['_2'].cast(\"double\")).drop('_2')\n",
    "tDf=tDf.withColumn(\"weight\",tDf['_3'].cast(\"double\")).drop('_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 65.78, 112.99]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=1.0, weight=65.78, height=112.99)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tDf=spark.createDataFrame(tRdd,[\"id\",\"weight\",\"height\"])\n",
    "tDf.printSchema()\n",
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 65.78  71.52  69.4   68.22  67.79]\n",
      "[ 112.99  136.49  153.03  142.34  144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "print np.array(_weightRdd)[:5]\n",
    "print np.array(_heightRdd)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x63018d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'o')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "           \"examples/src/main/resources/people.json\")\n",
    "\n",
    "_myDf= spark.read.json(jfile)\n",
    "_myDf.filter(_myDf['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print type(wc), type(wc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       " u'ClubCountry': u'Argentina',\n",
       " u'Competition': u'World Cup',\n",
       " u'DateOfBirth': u'1905-5-5',\n",
       " u'FullName': u'\\xc3ngel Bossio',\n",
       " u'IsCaptain': False,\n",
       " u'Number': u'',\n",
       " u'Position': u'GK',\n",
       " u'Team': u'Argentina',\n",
       " u'Year': 1930}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\400T6B\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\pyspark\\sql\\session.py:316: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=spark.createDataFrame(wc)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'Club': u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada',\n",
       "  u'ClubCountry': u'Argentina',\n",
       "  u'Competition': u'World Cup',\n",
       "  u'DateOfBirth': u'1905-5-5',\n",
       "  u'FullName': u'\\xc3ngel Bossio',\n",
       "  u'IsCaptain': False,\n",
       "  u'Number': u'',\n",
       "  u'Position': u'GK',\n",
       "  u'Team': u'Argentina',\n",
       "  u'Year': 1930}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)\n",
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\400T6B\\Downloads\\spark-2.0.0-bin-hadoop2.6\\python\\pyspark\\sql\\session.py:336: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "wcDF=spark.createDataFrame(wcRdd)\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print datetime.strptime(\"11/25/1991\", '%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubCountry: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Club=u'Club Atl\\xc3\\xa9tico Talleres de Remedios de Escalada', ClubCountry=u'Argentina', Competition=u'World Cup', DateOfBirth=u'1905-5-5', FullName=u'\\xc3ngel Bossio', IsCaptain=False, Number=u'', Position=u'GK', Team=u'Argentina', Year=1930, DoB=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcDF=wcDF.withColumn('DoB', wcDF['DateOfBirth'].cast(DateType()))\n",
    "wcDF=wcDF.withColumn('NumberInt', wcDF['Number'].cast(\"integer\"))\n",
    "wcDF.printSchema()\n",
    "wcDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.write.parquet(os.path.join(\"data\",\"people.parquet\"))\n",
    "_pDf=spark.read.parquet(os.path.join(\"data\",\"people.parquet\"))\n",
    "_pDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF=wcDF.withColumnRenamed('ClubCountry','ClubNation')\n",
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name=u'kim, js'),\n",
       " Row(name=u'lee, sm'),\n",
       " Row(name=u'lim, yg'),\n",
       " Row(name=u'lee')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'kim', u' js']\n"
     ]
    }
   ],
   "source": [
    "r=Row(name=u'kim, js')\n",
    "rd=r.asDict()\n",
    "print rd.values()[0].split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n",
      "+----+-----------+\n",
      "|year|max(height)|\n",
      "+----+-----------+\n",
      "|   1|        175|\n",
      "|   2|        180|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()\n",
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x),DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\",toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print int('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n",
      "+----+-------+------+-------+-----+\n",
      "|year|   name|height|heightD|yearI|\n",
      "+----+-------+------+-------+-----+\n",
      "|   1|kim, js|   170|  170.0|    1|\n",
      "|   1|lee, sm|   175|  175.0|    1|\n",
      "|   2|lim, yg|   180|  180.0|    2|\n",
      "|   2|    lee|   170|  170.0|    2|\n",
      "+----+-------+------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.types import IntegerType\n",
    "toint=udf(lambda x:int(x),IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\",toint(myDf['year']))\n",
    "\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+---------+\n",
      "|year|   name|height|heightD|yearI|nameUpper|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "|   1|kim, js|   170|  170.0|    1|  KIM, JS|\n",
      "|   1|lee, sm|   175|  175.0|    1|  LEE, SM|\n",
      "|   2|lim, yg|   180|  180.0|    2|  LIM, YG|\n",
      "|   2|    lee|   170|  170.0|    2|      LEE|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    " \n",
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())\n",
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+---------+----------+\n",
      "|year|   name|height|heightD|yearI|nameUpper|height>175|\n",
      "+----+-------+------+-------+-----+---------+----------+\n",
      "|   1|kim, js|   170|  170.0|    1|  KIM, JS|   shorter|\n",
      "|   1|lee, sm|   175|  175.0|    1|  LEE, SM|    taller|\n",
      "|   2|lim, yg|   180|  180.0|    2|  LIM, YG|    taller|\n",
      "|   2|    lee|   170|  170.0|    2|      LEE|   shorter|\n",
      "+----+-------+------+-------+-----+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))\n",
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|            height|           heightD|             yearI|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|                 4|                 4|                 4|\n",
      "|   mean|            173.75|            173.75|               1.5|\n",
      "| stddev|4.7871355387816905|4.7871355387816905|0.5773502691896257|\n",
      "|    min|               170|             170.0|                 1|\n",
      "|    max|               180|             180.0|                 2|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|yearI|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('yearI').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Club: string (nullable = true)\n",
      " |-- ClubNation: string (nullable = true)\n",
      " |-- Competition: string (nullable = true)\n",
      " |-- DateOfBirth: string (nullable = true)\n",
      " |-- FullName: string (nullable = true)\n",
      " |-- IsCaptain: boolean (nullable = true)\n",
      " |-- Number: string (nullable = true)\n",
      " |-- Position: string (nullable = true)\n",
      " |-- Team: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- DoB: date (nullable = true)\n",
      " |-- NumberInt: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----+\n",
      "|                Club|     Team|Year|\n",
      "+--------------------+---------+----+\n",
      "|Club Atltico Ta...|Argentina|1930|\n",
      "+--------------------+---------+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name=u'wc', database=None, description=None, tableType=u'TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcDF.createOrReplaceTempView(\"wc\")\n",
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full name: ngel Bossio\n",
      "Full name: Juan Botasso\n",
      "Full name: Roberto Cherro\n",
      "Full name: Alberto Chividini\n",
      "Full name: \n"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketDf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId,explode(bucketDf.items).alias('item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"apple\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|     apple|    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|itemId|      item|bucketId|\n",
      "+------+----------+--------+\n",
      "|    F5|   bananas|       2|\n",
      "|    F1|    orange|       1|\n",
      "|    F2|     apple|       1|\n",
      "|    F2|     apple|       2|\n",
      "|    F3| pineapple|       1|\n",
      "|    F4|watermelon|       2|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+-------+-----+---------+\n",
      "|year|   name|height|heightD|yearI|nameUpper|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "|   1|kim, js|   170|  170.0|    1|  KIM, JS|\n",
      "|   1|lee, sm|   175|  175.0|    1|  LEE, SM|\n",
      "|   2|lim, yg|   180|  180.0|    2|  LIM, YG|\n",
      "|   2|    lee|   170|  170.0|    2|      LEE|\n",
      "+----+-------+------+-------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\")\\\n",
    "            .otherwise(\"<175\").alias(\"how tall\"))\n",
    "_myDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(height)=173.75, avg(heightD)=173.75, avg(yearI)=1.5)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.groupBy().avg().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')\n",
    "if(not os.path.exists(_fname)):\n",
    "    print \"%s data does not exist! retrieving..\" % _fname\n",
    "    _f=urllib.urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494021"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0,tcp,http,SF,181,5450,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,9,9,1.00,0.00,0.11,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,239,486,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,19,19,1.00,0.00,0.05,0.00,0.00,0.00,0.00,0.00,normal.',\n",
       " u'0,tcp,http,SF,235,1337,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,8,8,0.00,0.00,0.00,0.00,1.00,0.00,0.00,29,29,1.00,0.00,0.03,0.00,0.00,0.00,0.00,0.00,normal.']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n"
     ]
    }
   ],
   "source": [
    "_normal = _rdd.filter(lambda x: 'normal.' in x)\n",
    "print _normal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_csvRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'0', u'tcp', u'http', u'SF', u'181', u'5450', u'0', u'0', u'0', u'0', u'0', u'1', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'0', u'8', u'8', u'0.00', u'0.00', u'0.00', u'0.00', u'1.00', u'0.00', u'0.00', u'9', u'9', u'1.00', u'0.00', u'0.11', u'0.00', u'0.00', u'0.00', u'0.00', u'0.00', u'normal.']]\n"
     ]
    }
   ],
   "source": [
    "print _csvRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_kv = _csvRdd.map(lambda x: (x[41], 1))\n",
    "_attack = _kv.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'guess_passwd.', 53),\n",
       " (u'nmap.', 231),\n",
       " (u'warezmaster.', 20),\n",
       " (u'rootkit.', 10),\n",
       " (u'warezclient.', 1020),\n",
       " (u'smurf.', 280790),\n",
       " (u'pod.', 264),\n",
       " (u'neptune.', 107201),\n",
       " (u'normal.', 97278),\n",
       " (u'spy.', 2),\n",
       " (u'ftp_write.', 8),\n",
       " (u'phf.', 4),\n",
       " (u'portsweep.', 1040),\n",
       " (u'teardrop.', 979),\n",
       " (u'buffer_overflow.', 30),\n",
       " (u'land.', 21),\n",
       " (u'imap.', 12),\n",
       " (u'loadmodule.', 9),\n",
       " (u'perl.', 3),\n",
       " (u'multihop.', 7),\n",
       " (u'back.', 2203),\n",
       " (u'ipsweep.', 1247),\n",
       " (u'satan.', 1589)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_attack.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_normalRdd=_csvRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_csvRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97278\n",
      "396743\n"
     ]
    }
   ],
   "source": [
    "print _normalRdd.count()\n",
    "print _attackRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'back.': (2203, 2203),\n",
       " u'buffer_overflow.': (30, 30),\n",
       " u'ftp_write.': (8, 8),\n",
       " u'guess_passwd.': (53, 53),\n",
       " u'imap.': (12, 12),\n",
       " u'ipsweep.': (1247, 1247),\n",
       " u'land.': (21, 21),\n",
       " u'loadmodule.': (9, 9),\n",
       " u'multihop.': (7, 7),\n",
       " u'neptune.': (107201, 107201),\n",
       " u'nmap.': (231, 231),\n",
       " u'normal.': (97278, 97278),\n",
       " u'perl.': (3, 3),\n",
       " u'phf.': (4, 4),\n",
       " u'pod.': (264, 264),\n",
       " u'portsweep.': (1040, 1040),\n",
       " u'rootkit.': (10, 10),\n",
       " u'satan.': (1589, 1589),\n",
       " u'smurf.': (280790, 280790),\n",
       " u'spy.': (2, 2),\n",
       " u'teardrop.': (979, 979),\n",
       " u'warezclient.': (1020, 1020),\n",
       " u'warezmaster.': (20, 20)}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_counts = _kv.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")\n",
    "\n",
    "sum_counts.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5])\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(_csvRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_df=spark.createDataFrame(_csvRdd)\n",
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+\n",
      "|protocol| count|\n",
      "+--------+------+\n",
      "|     tcp|190065|\n",
      "|     udp| 20354|\n",
      "|    icmp|283602|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select(\"protocol\", \"duration\", \"dst_bytes\").groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|protocol|count|\n",
      "+--------+-----+\n",
      "|     tcp|  139|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcp_interactions = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|duration|dst_bytes|\n",
      "+--------+---------+\n",
      "|    5057|        0|\n",
      "|    5059|        0|\n",
      "|    5051|        0|\n",
      "|    5056|        0|\n",
      "|    5051|        0|\n",
      "|    5039|        0|\n",
      "|    5062|        0|\n",
      "|    5041|        0|\n",
      "|    5056|        0|\n",
      "|    5064|        0|\n",
      "|    5043|        0|\n",
      "|    5061|        0|\n",
      "|    5049|        0|\n",
      "|    5061|        0|\n",
      "|    5048|        0|\n",
      "|    5047|        0|\n",
      "|    5044|        0|\n",
      "|    5063|        0|\n",
      "|    5068|        0|\n",
      "|    5062|        0|\n",
      "+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tcp_interactions_out = tcp_interactions.rdd\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5043, Dest. bytes: 0\n",
      "Duration: 5046, Dest. bytes: 0\n",
      "Duration: 5051, Dest. bytes: 0\n",
      "Duration: 5057, Dest. bytes: 0\n",
      "Duration: 5063, Dest. bytes: 0\n",
      "Duration: 42448, Dest. bytes: 0\n",
      "Duration: 40121, Dest. bytes: 0\n",
      "Duration: 31709, Dest. bytes: 0\n",
      "Duration: 30619, Dest. bytes: 0\n",
      "Duration: 22616, Dest. bytes: 0\n",
      "Duration: 21455, Dest. bytes: 0\n",
      "Duration: 13998, Dest. bytes: 0\n",
      "Duration: 12933, Dest. bytes: 0\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print ti_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
